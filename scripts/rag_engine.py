# -*- coding: utf-8 -*-
"""rag_engine.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zGvggYppW1vtSu9shnWYNTKV-xkE-qZe
"""



# scripts/rag_engine.py

from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
import subprocess
import spacy

# Função que garante que o modelo spaCy está instalado
def carregar_spacy_model():
    try:
        return spacy.load("pt_core_news_sm")
    except OSError:
        subprocess.run(["python", "-m", "spacy", "download", "pt_core_news_sm"])
        return spacy.load("pt_core_news_sm")

# Carrega o modelo de NLP
nlp = carregar_spacy_model()

# Extrai palavras-chave de uma pergunta
def extrair_keywords(texto):
    doc = nlp(texto)
    return list(set([ent.text for ent in doc.ents if len(ent.text) > 3]))

# Reescreve a pergunta com o LLM (opcional)
def reescrever_pergunta(pergunta, usar_llm=True):
    if not usar_llm:
        return pergunta
    try:
        llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo")
        prompt = f"""
Reescreva a pergunta abaixo de forma mais técnica, clara e compatível com os termos utilizados em documentos clínicos do SUS e no PCDT HIV:

\"{pergunta}\"
"""
        return llm.invoke(prompt).content.strip()
    except Exception as e:
        print(f"[!] Falha ao usar LLM para reescrever pergunta: {e}")
        return pergunta

# Carrega o mecanismo RAG
def load_rag_engine(faiss_path: str):
    embeddings = OpenAIEmbeddings()

    # Carrega a base vetorial FAISS (persistida localmente)
    vectorstore = FAISS.load_local(
        folder_path=faiss_path,
        embeddings=embeddings,
        allow_dangerous_deserialization=True  # necessário no Streamlit Cloud
    )

    retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
    llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo")

    prompt_template = PromptTemplate(
        input_variables=["context", "pergunta"],
        template="""
Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Use three sentences maximum and keep the answer as concise as possible.

{context}
Pergunta: {pergunta}
Resposta:
"""
    )

    chain = create_stuff_documents_chain(
        llm=llm,
        prompt=prompt_template,
        document_variable_name="context"
    )

    return chain, retriever, reescrever_pergunta
