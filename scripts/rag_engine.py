# -*- coding: utf-8 -*-
"""rag_engine.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zGvggYppW1vtSu9shnWYNTKV-xkE-qZe
"""



# scripts/rag_engine.py

from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
import subprocess
import spacy

# Extrai palavras-chave usando spaCy com fallback
def extrair_keywords(texto):
    try:
        nlp = spacy.load("pt_core_news_sm")
    except OSError:
        subprocess.run(["python", "-m", "spacy", "download", "pt_core_news_sm"])
        nlp = spacy.load("pt_core_news_sm")

    doc = nlp(texto)
    return list(set([ent.text for ent in doc.ents if len(ent.text) > 3]))

# Reescreve pergunta usando OpenAI LLM
def reescrever_pergunta(pergunta, usar_llm=True):
    if not usar_llm:
        return pergunta
    try:
        llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo")
        prompt = f"""
Reescreva a pergunta abaixo de forma mais técnica, clara e compatível com os termos utilizados em documentos clínicos do SUS e no PCDT HIV:

\"{pergunta}\"
"""
        return llm.invoke(prompt).content.strip()
    except Exception as e:
        print(f"[!] Falha ao usar LLM para reescrever pergunta: {e}")
        return pergunta

# Carrega a base vetorial FAISS e cria o chain
def load_rag_engine(faiss_path: str):
    embeddings = OpenAIEmbeddings()

    vectorstore = FAISS.load_local(
        folder_path=faiss_path,
        embeddings=embeddings,
        allow_dangerous_deserialization=True
    )

    retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
    llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo")

    prompt_template = PromptTemplate(
        input_variables=["context", "pergunta"],
        template="""
Use os trechos abaixo do PCDT para responder a pergunta de forma clara e objetiva.

{context}
Pergunta: {pergunta}
Resposta:
"""
    )

    chain = create_stuff_documents_chain(
        llm=llm,
        prompt=prompt_template,
        document_variable_name="context"
    )

    return chain, retriever, reescrever_pergunta
