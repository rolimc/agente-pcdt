# -*- coding: utf-8 -*-
"""rag_engine.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zGvggYppW1vtSu9shnWYNTKV-xkE-qZe
"""



# scripts/rag_engine.py

from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain

def extrair_keywords(texto):
    """
    Extrai palavras-chave com spaCy (opcional).
    """
    try:
        import spacy
        nlp = spacy.load("pt_core_news_sm")
        doc = nlp(texto)
        return list(set([ent.text for ent in doc.ents if len(ent.text) > 3]))
    except Exception as e:
        print(f"[!] spaCy não disponível ou erro ao carregar modelo: {e}")
        return []

def reescrever_pergunta(pergunta, usar_llm=True):
    """
    Reescreve a pergunta para torná-la mais técnica e clara com ajuda do LLM.
    """
    if not usar_llm:
        return pergunta

    try:
        llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo")
        prompt = f"""
Reescreva a pergunta abaixo de forma mais técnica, clara e compatível com os termos utilizados em documentos clínicos do SUS e no PCDT HIV:

\"{pergunta}\"
"""
        resposta = llm.invoke(prompt)
        return resposta.content.strip()  # ✅ Corrigido aqui
    except Exception as e:
        print(f"[!] Falha ao usar LLM para reescrever pergunta: {e}")
        return pergunta

def load_rag_engine(chroma_path: str):
    """
    Carrega a chain RAG com embeddings e base vetorial persistida.
    """
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma(
        persist_directory=chroma_path,
        embedding_function=embeddings
    )
    retriever = vectorstore.as_retriever(search_kwargs={"k": 40})

    llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo")

    prompt_template = PromptTemplate(
        input_variables=["context", "pergunta"],
        template="""
Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Use three sentences maximum and keep the answer as concise as possible.

{context}
Pergunta: {pergunta}
Resposta:
"""
    )

    chain = create_stuff_documents_chain(
        llm=llm,
        prompt=prompt_template,
        document_variable_name="context"
    )

    return chain, retriever, reescrever_pergunta
